{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-sol",
   "metadata": {},
   "source": [
    "# Solutions: Data Science & AI Engineer Interview Drills\n",
    "Reference implementations and discussion. Try the exercises yourself first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "easy-sol-header",
   "metadata": {},
   "source": [
    "## Easy Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "easy-sol1",
   "metadata": {},
   "source": [
    "### 1) Z-Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "easy-sol1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def zscore(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x)\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "    if std == 0:\n",
    "        return np.zeros_like(x, dtype=float)\n",
    "    return (x - mean) / std\n",
    "\n",
    "# Sanity check\n",
    "x = np.random.randn(1000)\n",
    "out = zscore(x)\n",
    "assert np.allclose(out.mean(), 0, atol=1e-2)\n",
    "assert np.allclose(out.std(), 1, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "easy-sol2",
   "metadata": {},
   "source": [
    "### 2) Missing-Value Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "easy-sol2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def impute_with_median(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    result = df.copy()\n",
    "    for col in cols:\n",
    "        if not np.issubdtype(result[col].dropna().dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "        median = result[col].median()\n",
    "        result[col] = result[col].fillna(median)\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "data = pd.DataFrame({\"a\": [1, 2, np.nan], \"b\": [3.0, np.nan, 5.0]})\n",
    "assert impute_with_median(data, [\"a\", \"b\"]).isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-sol-header",
   "metadata": {},
   "source": [
    "## Medium Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-sol1",
   "metadata": {},
   "source": [
    "### 3) Sliding-Window Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-sol1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(x: np.ndarray, k: int) -> np.ndarray:\n",
    "    x = np.asarray(x)\n",
    "    if k <= 0 or k > x.shape[0]:\n",
    "        raise ValueError(\"Invalid window size\")\n",
    "    stride = x.strides[0]\n",
    "    shape = (x.shape[0] - k + 1, k)\n",
    "    return np.lib.stride_tricks.as_strided(x, shape=shape, strides=(stride, stride))\n",
    "\n",
    "# Check\n",
    "x = np.arange(6)\n",
    "expected = np.array([[0,1,2],[1,2,3],[2,3,4],[3,4,5]])\n",
    "assert np.array_equal(sliding_windows(x,3), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-sol2",
   "metadata": {},
   "source": [
    "### 4) Custom AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-sol2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def binary_auc(y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "    if y_true.shape != y_score.shape:\n",
    "        raise ValueError(\"Shapes must match\")\n",
    "    if not set(np.unique(y_true)).issubset({0, 1}):\n",
    "        raise ValueError(\"y_true must be binary\")\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    # Trapezoidal integration\n",
    "    return np.trapz(tpr, fpr)\n",
    "\n",
    "# Check against sklearn\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_score = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "assert np.isclose(binary_auc(y_true, y_score), 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-sol3",
   "metadata": {},
   "source": [
    "### 5) Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-sol3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def _add_bias(X: np.ndarray) -> np.ndarray:\n",
    "    return np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "\n",
    "def predict_proba(X: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    Xb = _add_bias(X)\n",
    "    return sigmoid(Xb @ w)\n",
    "\n",
    "def loss(X: np.ndarray, y: np.ndarray, w: np.ndarray, lambda_: float = 0.0) -> float:\n",
    "    Xb = _add_bias(X)\n",
    "    logits = Xb @ w\n",
    "    eps = 1e-9\n",
    "    ll = y * np.log(sigmoid(logits) + eps) + (1 - y) * np.log(1 - sigmoid(logits) + eps)\n",
    "    reg = 0.5 * lambda_ * np.sum(w[1:] ** 2)\n",
    "    return -ll.mean() + reg\n",
    "\n",
    "def fit_logreg(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    lr: float = 0.1,\n",
    "    epochs: int = 1000,\n",
    "    lambda_: float = 0.0,\n",
    "    tol: float = 1e-6,\n",
    ") -> np.ndarray:\n",
    "    Xb = _add_bias(X)\n",
    "    w = np.zeros(Xb.shape[1])\n",
    "    prev_loss = np.inf\n",
    "    for _ in range(epochs):\n",
    "        preds = sigmoid(Xb @ w)\n",
    "        grad = Xb.T @ (preds - y) / len(y)\n",
    "        grad[1:] += lambda_ * w[1:]\n",
    "        w -= lr * grad\n",
    "        cur_loss = loss(X, y, w, lambda_)\n",
    "        if abs(prev_loss - cur_loss) < tol:\n",
    "            break\n",
    "        prev_loss = cur_loss\n",
    "    return w\n",
    "\n",
    "# Quick sanity check\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=200, n_features=4, random_state=42)\n",
    "w = fit_logreg(X, y, lr=0.1, epochs=5000, lambda_=0.1)\n",
    "preds = predict_proba(X, w) >= 0.5\n",
    "assert (preds == y).mean() > 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expert-sol-header",
   "metadata": {},
   "source": [
    "## Expert Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expert-sol1",
   "metadata": {},
   "source": [
    "### 6) Minimal MLP in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expert-sol1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int = 32, p_drop: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device: str = \"cpu\") -> float:\n",
    "    model.train()\n",
    "    total_loss, total_count = 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_count += xb.size(0)\n",
    "    return total_loss / total_count\n",
    "\n",
    "def evaluate(model, loader, criterion, device: str = \"cpu\") -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_correct += (preds == yb).sum().item()\n",
    "            total_count += xb.size(0)\n",
    "    return total_loss / total_count, total_correct / total_count\n",
    "\n",
    "# Smoke test\n",
    "set_seed(42)\n",
    "X = torch.randn(200, 10)\n",
    "y = (X.sum(dim=1) + 0.2 * torch.randn(200) > 0).float()\n",
    "loader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)\n",
    "model = MLP(in_dim=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "for epoch in range(3):\n",
    "    train_loss = train_epoch(model, loader, optimizer, criterion)\n",
    "val_loss, acc = evaluate(model, loader, criterion)\n",
    "assert acc > 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expert-sol2",
   "metadata": {},
   "source": [
    "### 7) Transformer Block Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expert-sol2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, dim_ff: int, p_drop: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, dropout=p_drop, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(dim_ff, d_model),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pre-norm + self-attention\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, need_weights=False)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        # Pre-norm + feed-forward\n",
    "        ff_norm = self.norm2(x)\n",
    "        ff_out = self.ffn(ff_norm)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        return x\n",
    "\n",
    "# Check\n",
    "blk = TransformerBlock(d_model=32, nhead=4, dim_ff=64)\n",
    "dummy = torch.randn(2, 5, 32)\n",
    "out = blk(dummy)\n",
    "assert out.shape == dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expert-sol3",
   "metadata": {},
   "source": [
    "### 8) Offline Metrics vs. Online Metrics (Design)\n",
    "- **Offline:**\n",
    "  - `AUC/ROC` for ranking quality; insensitive to threshold.\n",
    "  - `NDCG@k` to emphasize top-k relevance (aligns with recommender objectives).\n",
    "  - `Calibration error` (ECE/Brier) to ensure probabilities are trustworthy for downstream decisions.\n",
    "- **Online (A/B):**\n",
    "  - `CTR`/`conversion` lift and `add-to-cart`/`play` rate as primary success metrics.\n",
    "  - `Latency`/`p95 response` and `error rate` as guardrails; also watch `bounce rate` or `session length` for negative UX shifts.\n",
    "- **Canary rollout:**\n",
    "  - Start with small traffic slice (e.g., 1â€“5%), region/segment isolated; compare to control in real time.\n",
    "  - Automatic rollback if guardrails breached; gradually ramp traffic with monitoring windows.\n",
    "  - Keep feature flags to disable rapidly; log inputs/outputs for postmortem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expert-sol4",
   "metadata": {},
   "source": [
    "### 9) Feature Store Consistency (Design)\n",
    "- **Point-in-time correctness:** enforce event timestamps, use as-of joins in offline pipelines to avoid future leakage; freeze lookup times for training sets.\n",
    "- **Backfills/versioning:** immutably store feature values with data/version stamps; backfill via new versions rather than overwriting; track lineage in metadata.\n",
    "- **Validation/monitoring:** schema checks (types, ranges), null/volume drift alerts, training/serving distribution comparisons for key features; canary new feature versions before full rollout.\n",
    "- **Operational playbooks:** feature flagging to disable problematic features, rollback to prior version, alerting channels, and runbooks for re-materialization; keep golden datasets for quick verification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
