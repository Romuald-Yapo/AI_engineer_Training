{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Data Science & AI Engineer Interview Drills\n",
    "\n",
    "Sharpen core skills across statistics, data wrangling, modeling, and deep learning with hands-on, interview-style exercises. Each section increases in difficulty (Easy → Medium → Expert)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructions",
   "metadata": {},
   "source": [
    "## How to use this notebook\n",
    "- Work through exercises in order; time-box yourself like an interview (10–20 min each).\n",
    "- Avoid looking at hints immediately; try to verbalize your approach first.\n",
    "- Write clean, tested code; add assertions/printouts to validate assumptions.\n",
    "- Use standard libraries (`numpy`, `pandas`, `scikit-learn`, `torch`) as needed, but implement the logic yourself unless specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "easy-header",
   "metadata": {},
   "source": [
    "## Easy: Foundations\n",
    "Warm up with vectorized thinking, basic stats, and quick data checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "easy-ex1",
   "metadata": {},
   "source": [
    "### 1) Z-Score Normalization (NumPy)\n",
    "Given a 1D NumPy array `x`, return a normalized array `(x - mean) / std`.\n",
    "\n",
    "**Constraints:**\n",
    "- Avoid Python loops; use vectorized operations.\n",
    "- Handle the case `std == 0` by returning zeros.\n",
    "\n",
    "**Test yourself:** Verify mean≈0 and std≈1 on random inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "easy-ex1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def zscore(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return z-scored version of x. Handle zero std by returning zeros.\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Quick checks (uncomment as you work)\n",
    "# x = np.random.randn(1000)\n",
    "# out = zscore(x)\n",
    "# print(out.mean(), out.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "easy-ex2",
   "metadata": {},
   "source": [
    "### 2) Missing-Value Imputer (Pandas)\n",
    "Implement `impute_with_median(df, cols)` that replaces `NaN` in specified columns with the column median.\n",
    "\n",
    "**Constraints:**\n",
    "- Do not mutate the input DataFrame; return a copy.\n",
    "- Columns may be non-numeric; raise a `ValueError` if median cannot be computed.\n",
    "- Preserve dtypes.\n",
    "\n",
    "**Follow-up:** How would you handle grouped medians (e.g., per category)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "easy-ex2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def impute_with_median(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Return a copy with NaNs in cols replaced by median values.\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Example usage\n",
    "# data = pd.DataFrame({\"a\": [1, 2, np.nan], \"b\": [3.0, np.nan, 5.0]})\n",
    "# print(impute_with_median(data, [\"a\", \"b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-header",
   "metadata": {},
   "source": [
    "## Medium: Classical ML & Evaluation\n",
    "Practice feature engineering, model evaluation, and algorithmic thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-ex1",
   "metadata": {},
   "source": [
    "### 3) Sliding-Window Feature Extraction (NumPy)\n",
    "Given a 1D array `x` and window size `k`, generate a 2D array of shape `(len(x) - k + 1, k)` where each row is a contiguous window.\n",
    "\n",
    "**Constraints:**\n",
    "- Use stride tricks (`np.lib.stride_tricks.as_strided`) or vectorization; avoid Python loops.\n",
    "- Raise `ValueError` if `k` is invalid.\n",
    "\n",
    "**Test yourself:** Compare against a loop-based baseline for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-ex1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(x: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"Return strided 2D view of sliding windows of length k.\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError\n",
    "\n",
    "# x = np.arange(6)\n",
    "# print(sliding_windows(x, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-ex2",
   "metadata": {},
   "source": [
    "### 4) Custom AUC (Scikit-Learn)\n",
    "Implement `binary_auc(y_true, y_score)` without using `roc_auc_score`. You may use NumPy but implement the trapezoidal integration yourself.\n",
    "\n",
    "**Constraints:**\n",
    "- Handle ties in scores robustly.\n",
    "- Validate input shapes and value ranges.\n",
    "\n",
    "**Follow-up:** Discuss how class imbalance affects AUC interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-ex2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def binary_auc(y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
    "    \"\"\"Compute AUC via ROC curve and trapezoidal rule, without roc_auc_score.\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError\n",
    "\n",
    "# y_true = np.array([0, 0, 1, 1])\n",
    "# y_score = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "# print(binary_auc(y_true, y_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-ex3",
   "metadata": {},
   "source": [
    "### 5) Logistic Regression from Scratch (Binary)\n",
    "Implement logistic regression using batch gradient descent.\n",
    "\n",
    "**Requirements:**\n",
    "- Functions: `sigmoid(z)`, `predict_proba(X, w)`, `loss(X, y, w)`, `fit_logreg(X, y, lr, epochs)` returning weights.\n",
    "- Add L2 regularization (hyperparameter `lambda_`).\n",
    "- Stop early if loss plateaus.\n",
    "\n",
    "**Test yourself:** Compare learned weights to `sklearn.linear_model.LogisticRegression` on a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-ex3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "def predict_proba(X: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "def loss(X: np.ndarray, y: np.ndarray, w: np.ndarray, lambda_: float = 0.0) -> float:\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "def fit_logreg(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    lr: float = 0.1,\n",
    "    epochs: int = 1000,\n",
    "    lambda_: float = 0.0,\n",
    "    tol: float = 1e-6,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Train logistic regression with L2 penalty and early stopping. Return weights.\"\"\"\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "# # Example sanity check\n",
    "# from sklearn.datasets import make_classification\n",
    "# X, y = make_classification(n_samples=200, n_features=4, random_state=42)\n",
    "# w = fit_logreg(X, y, lr=0.1, epochs=5000, lambda_=0.1)\n",
    "# preds = predict_proba(X, w) >= 0.5\n",
    "# print((preds == y).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expert-header",
   "metadata": {},
   "source": [
    "## Expert: Deep Learning & ML Systems\n",
    "Apply PyTorch, optimization, and production-aware thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expert-ex1",
   "metadata": {},
   "source": [
    "### 6) Minimal MLP in PyTorch (Binary Classification)\n",
    "Build and train a two-layer MLP on synthetic data.\n",
    "\n",
    "**Requirements:**\n",
    "- Model: `Linear -> ReLU -> Dropout -> Linear`.\n",
    "- Use BCEWithLogitsLoss; track training loss and accuracy per epoch.\n",
    "- Implement `train_epoch` and `evaluate` loops without `torchvision` helpers.\n",
    "- Add deterministic seeding.\n",
    "\n",
    "**Follow-up:** Explain when to prefer `nn.BCEWithLogitsLoss` vs `nn.BCELoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expert-ex1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int = 32, p_drop: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device: str = \"cpu\") -> float:\n",
    "    \"\"\"One training epoch; return average loss.\"\"\"\n",
    "    # TODO: implement loop\n",
    "    raise NotImplementedError\n",
    "\n",
    "def evaluate(model, loader, criterion, device: str = \"cpu\") -> tuple[float, float]:\n",
    "    \"\"\"Evaluate; return (avg_loss, accuracy).\"\"\"\n",
    "    # TODO: implement loop\n",
    "    raise NotImplementedError\n",
    "\n",
    "# # Example synthetic run (uncomment to test)\n",
    "# set_seed(42)\n",
    "# X = torch.randn(1000, 10)\n",
    "# y = (X.sum(dim=1) + 0.2 * torch.randn(1000) > 0).float()\n",
    "# dataset = TensorDataset(X, y)\n",
    "# loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# model = MLP(in_dim=10)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# for epoch in range(5):\n",
    "#     train_loss = train_epoch(model, loader, optimizer, criterion)\n",
    "#     val_loss, acc = evaluate(model, loader, criterion)\n",
    "#     print(epoch, train_loss, val_loss, acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expert-ex2",
   "metadata": {},
   "source": [
    "### 7) Transformer Block Forward Pass (PyTorch)\n",
    "Implement a simplified Transformer encoder block forward pass with pre-layer normalization.\n",
    "\n",
    "**Components:**\n",
    "- Multi-head self-attention (`nn.MultiheadAttention`, batch-first).\n",
    "- Feed-forward network: `Linear -> GELU -> Dropout -> Linear`.\n",
    "- Residual connections & layer norms (pre-norm style).\n",
    "- Dropout on attention output and FFN output.\n",
    "\n",
    "**API:** `forward(x: Tensor) -> Tensor` where `x` has shape `(batch, seq, d_model)`.\n",
    "\n",
    "**Follow-up:** Discuss why pre-norm can stabilize training for deep stacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expert-ex2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, dim_ff: int, p_drop: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, dropout=p_drop, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(dim_ff, d_model),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: implement pre-norm residual block\n",
    "        raise NotImplementedError\n",
    "\n",
    "# # Sanity check\n",
    "# blk = TransformerBlock(d_model=32, nhead=4, dim_ff=64)\n",
    "# dummy = torch.randn(2, 5, 32)\n",
    "# out = blk(dummy)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expert-ex3",
   "metadata": {},
   "source": [
    "### 8) Offline Metrics vs. Online Metrics (Design)\n",
    "You are asked to ship a recommendation model trained offline. Describe:\n",
    "- Three offline metrics you would report and why.\n",
    "- Two online metrics (A/B test) and guardrail metrics.\n",
    "- How you would design a canary rollout to mitigate risk.\n",
    "\n",
    "Write concise bullets; aim for depth and clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expert-ex4",
   "metadata": {},
   "source": [
    "### 9) Feature Store Consistency (Design)\n",
    "Explain how you would ensure **training/serving skew** is minimized when using a feature store.\n",
    "\n",
    "Cover:\n",
    "- Point-in-time correctness.\n",
    "- Backfills and data versioning.\n",
    "- Validation/monitoring signals to catch drift or schema changes.\n",
    "- Operational playbooks when a breaking change occurs.\n",
    "\n",
    "Write a short, structured proposal (bullets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrap-up",
   "metadata": {},
   "source": [
    "## Tips for Interview Readiness\n",
    "- Articulate trade-offs (compute vs. latency, bias vs. variance, offline vs. online metrics).\n",
    "- Narrate your debugging steps; interviewers value clarity.\n",
    "- Add lightweight tests to prove correctness under edge cases.\n",
    "- Keep code modular and readable; prefer pure functions where possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
