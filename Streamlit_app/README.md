# LLM Medical Report Visualization & Pipeline Evaluation

An end-to-end experimentation platform for clinical concept extraction with Large Language Models (LLMs).  
The repository packages a Streamlit application, supporting pipelines, and synthetic datasets that make it
easy to configure models, test prompts, highlight evidence in medical reports, and benchmark extraction quality.

---

## Highlights

- üè• **Clinical report explorer** ‚Äì browse synthetic hospital stays, toggle between raw/clean versions, and highlight contexts returned by LLMs (`display_text.py`).
- ü§ñ **Pipeline composer** ‚Äì persist model settings, prompt versions, and anti-hallucination strategies inside a SQLite database (`initialize_pipeline_db.py`, `pipelines.db`).
- üß™ **Unit-test harness** ‚Äì run extractions on-the-fly with ensembles, chain-of-verification and judge models (`llm_extractor_pipeline.py`) before promoting a pipeline.
- üìä **Evaluation lab** ‚Äì upload multi-annotator labels, generate a gold standard, and compute per-concept metrics through `evaluation_pipeline.py`.
- üß∑ **Reproducible demo data** ‚Äì regenerate the full synthetic dataset (`synthetic_data/*.parquet`) and sample annotation CSV files to stress-test the UI.

---

## Repository Map

| Path | Description |
| --- | --- |
| `app_last_version.py` | Main Streamlit entry-point with pipeline configuration, unit tests, and evaluation tabs. |
| `display_text.py` | Helpers for AgGrid tables and fuzzy context highlighting inside reports. |
| `llm_extractor_pipeline.py` | LangChain/Ollama based ensemble extractor with retries, anti-hallucination, and judge logic. |
| `evaluation_pipeline.py` | Batched projection helper plus precision/recall/F1 computation for any concept list. |
| `initialize_pipeline_db.py` | Creates `pipelines.db` with `pipelines`, `models`, `prompts`, and `pipeline_models` tables. |
| `synthetic_data/*.parquet` | Patients, stays, and text documents powering the Streamlit demo. |
| `generate_synthetic_data.py` | Rebuilds the synthetic parquet fixtures. |
| `generate_synthetic_report_annotation.py` | Creates three reference annotator CSV files for evaluation demos. |
| `annotator*_medical_concepts.csv` | Sample gold-standard style labels (Diab√®te, HTA, IC, BPCO) generated by the script above. |
| `test_app.py` | Lightweight Streamlit harness for experimenting with annotation persistence. |
| `start.sh` / `stop.sh` | Convenience scripts for running the Streamlit app in headless deployments. |
| `*.ipynb` | Exploratory notebooks for database analysis and evaluation prototyping. |

---

## Requirements

- Python 3.11+ (the project is regularly tested on 3.12)
- Streamlit 1.34+
- Pandas, Polars, Plotly, `st-aggrid`
- LangChain (`langchain`, `langchain-core`, `langchain-community`, `langchain-ollama`)
- `fuzzywuzzy` (optionally `python-Levenshtein` for speed)
- `requests`, `sqlite3` (stdlib), `numpy`
- Access to an [Ollama](https://ollama.com) server hosting the clinical extraction model(s)

A minimal installation snippet:

```bash
python -m venv .venv
. .venv/Scripts/activate  # PowerShell: .\.venv\Scripts\Activate.ps1
pip install --upgrade pip
pip install streamlit pandas polars plotly st-aggrid \
            langchain langchain-core langchain-community langchain-ollama \
            fuzzywuzzy python-Levenshtein requests
```

> ‚ÑπÔ∏è `llm_extractor_pipeline.py` currently points to a specific Ollama base URL inside `load_model_ollama`. Update the `base_url` (or read it from `OLLAMA_BASE_URL`) if you are hosting the models elsewhere.

---

## Data & Database Setup

1. **Synthetic reports:**  
   Pre-generated parquet files sit in `synthetic_data/`. Regenerate them at any time:
   ```bash
   python generate_synthetic_data.py
   ```

2. **Annotation samples:**  
   The CSV files `annotator1_medical_concepts.csv`, `annotator2_medical_concepts.csv`, `annotator3_medical_concepts.csv` were produced via:
   ```bash
   python generate_synthetic_report_annotation.py
   ```
   They provide three slightly different viewpoints on the same 25 mock reports and are ideal for testing inter-rater agreement and adjudication workflows.

3. **Pipeline metadata (SQLite):**  
   The Streamlit app stores models, prompts, and pipeline definitions in `pipelines.db`.  
   Delete the file to reset, or recreate it explicitly:
   ```bash
   python initialize_pipeline_db.py
   ```
   The schema enforces uniqueness on model identifiers, validates ensemble settings, and timestamps updates automatically.

4. **Optional legacy DB:**  
   `init_database.py` creates an earlier schema (`llm_medical_data_extraction_version1.db`) kept for backwards compatibility.

---

## Running the Streamlit Application

```bash
streamlit run app_last_version.py --server.baseUrlPath=/ --logger.level=info
```

Use `start.sh`/`stop.sh` when deploying on Linux servers:

```bash
bash start.sh   # launches Streamlit with nohup
bash stop.sh    # handles process teardown
```

### UI Overview

1. **Pipeline Configuration tab**
   - `Models`: CRUD interface for Ollama model descriptors (name, identifier, decoding params, context window).
   - `Prompts`: Version prompts per type (technical/system vs. clinical/user instructions).
   - `Pipeline`: Assemble ordered model ensembles, toggle anti-hallucination strategies (ensemble vote, chain-of-verification reminders, contextual grounding, LLM-as-judge), and assign prompts/data sources.

2. **Pipeline Unit Test tab**
   - `Report Selection`: Browse `synthetic_data/text_documents.parquet` with AgGrid; choose a patient, select a document, and switch between cleaned text or raw HTML/XML views. The fuzzy highlighter (`display_text.py`) uses the contexts returned by the extractor to color the original report.
   - `Extraction`: Run the currently selected pipeline on the chosen report. Behind the scenes `llm_extractor_pipeline.py` retries malformed JSON, enforces output schemas, and aggregates ensembles.
   - `Raw Output`: Inspect each model response, prompt used, and any warnings, which is essential before promoting a pipeline to evaluation.

3. **Pipeline Evaluation tab**
   - `Annotation Upload`: Import CSV files (single or multiple annotators). The app automatically infers the ID column plus concept columns (binary labels).
   - `Gold Standard Builder`: Merge annotators into a consensus dataset, preview it in Streamlit, and export to `gold_standard.csv`.
   - `Evaluation Runner`: Choose train/test splits, select a saved pipeline, and call `evaluation_pipeline.run_pipeline_prediction`. Metrics are computed via `compute_concept_metrics`, optionally filtered by a judge model, and can be saved as JSON (`pipeline_performances.json`).
   - `Per-model insights`: Every evaluation stores individual model projections, aggregated predictions, and optional judge outputs for auditing.

Session state keeps track of uploaded annotations, UI selections, and temporary results so you can iterate quickly during a single Streamlit session.

---

## Programmatic Usage

Even without the UI, you can reuse the building blocks in scripts or notebooks:

```python
import pandas as pd
from evaluation_pipeline import run_pipeline_prediction, compute_concept_metrics

df = pd.read_csv("gold_standard.csv")
concepts = ["Diabete", "HTA", "IC", "BPCO"]

model_confs = [
    {"model": "mistral:7b-instruct", "temperature": 0.2, "num_ctx": 4096},
    {"model": "llama3:8b-instruct", "temperature": 0.1, "num_ctx": 8192},
]

results = run_pipeline_prediction(
    df=df,
    id_col="ID",
    text_col="report_text",
    concept_cols=concepts,
    model_confs=model_confs,
    system_prompt="Tu es un assistant d'extraction d'information clinique.",
    user_prompt="Analyse le rapport et retourne un JSON valide.",
    anti={"ensemble": True, "ensemble_size": 2, "llm_as_judge": False},
)

metrics_df, summary = compute_concept_metrics(df, results["aggregated"], concepts, id_col="ID")
print(metrics_df)
print(summary)
```

Use `llm_extractor_pipeline.build_messages` and `llm_extractor_pipeline.llm_extractor` if you need granular control over prompting, retries, or JSON validation for a single report.

---

## Annotation Utilities

- `test_app.py` holds a pared-down Streamlit interface for persisting uploaded annotations into `test_pipeline.db`.
- `evaluation_pipeline.py` exposes helper functions (`run_model_projection`, `aggregate_predictions`, `apply_judge_filter`) that you can import into notebooks like `llm_evalution_pipeline_test.ipynb`.

---

## Development Tips

- **Coding style:** the project sticks to plain Python modules; lint with `ruff` or `flake8` if you add scripts.
- **Testing:** because Streamlit callbacks dominate the app, prefer writing targeted unit tests for helper modules (`display_text`, `llm_extractor_pipeline`, `evaluation_pipeline`) before extending the UI.
- **LLM endpoints:** consider surfacing `base_url` and authentication settings through environment variables if you deploy beyond the intranet address currently hardcoded.
- **Data privacy:** synthetic data is included for convenience. Replace `synthetic_data/*.parquet` with de-identified hospital data and point the app to your secure storage before deploying in clinical environments.

---

## Roadmap / Ideas

1. Fine-tune or distil existing LLMs on the synthetic extraction task to reduce latency.
2. Inline highlighting of extracted spans directly into the Streamlit text viewer using similarity scoring.
3. Improve UI load times (e.g., caching DB lookups, paginating AgGrid) for larger corpora.
4. Add optional authentication in Streamlit before exposing the tool to clinicians.
5. Automate pipeline regression tests that run `evaluation_pipeline` nightly and append results into `pipeline_performances.json`.
