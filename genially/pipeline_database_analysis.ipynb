{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Database Exploration\n",
    "",
    "Inspect and analyze the saved pipeline configuration data stored in `pipelines.db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "DB_PATH = Path(\"pipelines.db\").resolve()\n",
    "print(f\"Using database at {DB_PATH}\")\n",
    "\n",
    "def get_connection():\n",
    "    return sqlite3.connect(DB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "    HAS_PANDAS = True\n",
    "    print(f\"pandas {pd.__version__} loaded\")\n",
    "except ImportError:\n",
    "    pd = None\n",
    "    display = None\n",
    "    HAS_PANDAS = False\n",
    "    print(\"pandas is not installed; results will be shown as plain records.\")\n",
    "\n",
    "def fetch_records(query, params=None, frame=None):\n",
    "    params = params or ()\n",
    "    with get_connection() as conn:\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        cur = conn.execute(query, params)\n",
    "        rows = cur.fetchall()\n",
    "    records = [dict(row) for row in rows]\n",
    "    if (frame or (frame is None and HAS_PANDAS)) and HAS_PANDAS:\n",
    "        df = pd.DataFrame.from_records(records)\n",
    "        if display:\n",
    "            display(df)\n",
    "        return df\n",
    "    for record in records:\n",
    "        print(record)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_overview = fetch_records(\n",
    "    \"\"\"\n",
    "    SELECT name,\n",
    "           type\n",
    "    FROM sqlite_master\n",
    "    WHERE type IN ('table','index','trigger')\n",
    "    ORDER BY type, name\n",
    "    \"\"\"\n",
    ")\n",
    "schema_overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View models table\n",
    "models_df = fetch_records(\n",
    "    \"\"\"\n",
    "    SELECT id,\n",
    "           name,\n",
    "           description,\n",
    "           identifier,\n",
    "           temperature,\n",
    "           top_p,\n",
    "           top_k,\n",
    "           num_ctx,\n",
    "           max_output,\n",
    "           created_at,\n",
    "           updated_at\n",
    "    FROM models\n",
    "    ORDER BY id\n",
    "    \"\"\"\n",
    ")\n",
    "models_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View prompts table\n",
    "prompts_df = fetch_records(\n",
    "    \"\"\"\n",
    "    SELECT id,\n",
    "           name,\n",
    "           description,\n",
    "           type,\n",
    "           content,\n",
    "           created_at,\n",
    "           updated_at\n",
    "    FROM prompts\n",
    "    ORDER BY id\n",
    "    \"\"\"\n",
    ")\n",
    "prompts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View pipelines table\n",
    "pipelines_df = fetch_records(\n",
    "    \"\"\"\n",
    "    SELECT id,\n",
    "           name,\n",
    "           description,\n",
    "           data_source,\n",
    "           technical_prompt_id,\n",
    "           clinical_prompt_id,\n",
    "           ensemble_enabled,\n",
    "           ensemble_size,\n",
    "           chain_of_verification,\n",
    "           contextual_grounding,\n",
    "           llm_as_judge,\n",
    "           judge_model_id,\n",
    "           created_at,\n",
    "           updated_at\n",
    "    FROM pipelines\n",
    "    ORDER BY id\n",
    "    \"\"\"\n",
    ")\n",
    "pipelines_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View pipeline steps with ordering\n",
    "pipeline_steps_df = fetch_records(\n",
    "    \"\"\"\n",
    "    SELECT p.id AS pipeline_id,\n",
    "           p.name,\n",
    "           pm.step_order,\n",
    "           pm.model_id,\n",
    "           pm.created_at\n",
    "    FROM pipelines AS p\n",
    "    LEFT JOIN pipeline_models AS pm\n",
    "      ON pm.pipeline_id = p.id\n",
    "    ORDER BY p.id, pm.step_order\n",
    "    \"\"\"\n",
    ")\n",
    "pipeline_steps_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not HAS_PANDAS:\n",
    "    print(\"Install pandas to unlock aggregated metrics.\")\n",
    "elif pipelines_df is None or pipelines_df.empty:\n",
    "    print(\"No pipelines recorded yet.\")\n",
    "else:\n",
    "    bool_cols = [\n",
    "        \"ensemble_enabled\",\n",
    "        \"chain_of_verification\",\n",
    "        \"contextual_grounding\",\n",
    "        \"llm_as_judge\",\n",
    "    ]\n",
    "    pipeline_metrics = pipelines_df.copy()\n",
    "    pipeline_metrics[bool_cols] = pipeline_metrics[bool_cols].astype(bool)\n",
    "\n",
    "    if isinstance(pipeline_steps_df, pd.DataFrame) and not pipeline_steps_df.empty:\n",
    "        step_counts = (\n",
    "            pipeline_steps_df[pipeline_steps_df[\"pipeline_id\"].notna()]\n",
    "            .groupby(\"pipeline_id\")[\"model_id\"]\n",
    "            .count()\n",
    "        )\n",
    "    else:\n",
    "        step_counts = pd.Series(dtype=\"int64\")\n",
    "\n",
    "    pipeline_metrics[\"model_steps\"] = (\n",
    "        pipeline_metrics[\"id\"].map(step_counts).fillna(0).astype(int)\n",
    "    )\n",
    "    pipeline_metrics[\"mitigation_count\"] = pipeline_metrics[bool_cols].sum(axis=1)\n",
    "\n",
    "    if isinstance(models_df, pd.DataFrame) and not models_df.empty:\n",
    "        model_lookup = models_df.set_index(\"id\")\n",
    "        pipeline_metrics[\"judge_model_identifier\"] = pipeline_metrics[\"judge_model_id\"].map(\n",
    "            model_lookup[\"identifier\"]\n",
    "        )\n",
    "    else:\n",
    "        pipeline_metrics[\"judge_model_identifier\"] = pipeline_metrics[\"judge_model_id\"]\n",
    "\n",
    "    if isinstance(prompts_df, pd.DataFrame) and not prompts_df.empty:\n",
    "        prompt_lookup = prompts_df.set_index(\"id\")[\"name\"]\n",
    "        pipeline_metrics[\"technical_prompt_name\"] = pipeline_metrics[\"technical_prompt_id\"].map(prompt_lookup)\n",
    "        pipeline_metrics[\"clinical_prompt_name\"] = pipeline_metrics[\"clinical_prompt_id\"].map(prompt_lookup)\n",
    "    else:\n",
    "        pipeline_metrics[\"technical_prompt_name\"] = pipeline_metrics[\"technical_prompt_id\"]\n",
    "        pipeline_metrics[\"clinical_prompt_name\"] = pipeline_metrics[\"clinical_prompt_id\"]\n",
    "\n",
    "    summary_stats = pd.DataFrame(\n",
    "        {\n",
    "            \"metric\": [\n",
    "                \"Pipelines\",\n",
    "                \"Average models per pipeline\",\n",
    "                \"Median models per pipeline\",\n",
    "                \"Pipelines with mitigations\",\n",
    "            ],\n",
    "            \"value\": [\n",
    "                len(pipeline_metrics),\n",
    "                pipeline_metrics[\"model_steps\"].mean().round(2),\n",
    "                pipeline_metrics[\"model_steps\"].median(),\n",
    "                int((pipeline_metrics[\"mitigation_count\"] > 0).sum()),\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    display(summary_stats)\n",
    "\n",
    "    strategy_totals = pipeline_metrics[bool_cols].sum().rename(\"pipelines_enabled\").to_frame()\n",
    "    strategy_totals.index.name = \"strategy\"\n",
    "    display(strategy_totals)\n",
    "\n",
    "    combinations = (\n",
    "        pipeline_metrics.groupby(bool_cols, dropna=False)[\"id\"]\n",
    "        .count()\n",
    "        .rename(\"pipeline_count\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"pipeline_count\", ascending=False)\n",
    "    )\n",
    "    display(combinations)\n",
    "\n",
    "    detail_cols = [\n",
    "        \"id\",\n",
    "        \"name\",\n",
    "        \"model_steps\",\n",
    "        \"mitigation_count\",\n",
    "        \"technical_prompt_name\",\n",
    "        \"clinical_prompt_name\",\n",
    "        \"judge_model_identifier\",\n",
    "        *bool_cols,\n",
    "        \"created_at\",\n",
    "        \"updated_at\",\n",
    "    ]\n",
    "    display(\n",
    "        pipeline_metrics[detail_cols]\n",
    "        .sort_values(\"updated_at\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if (\n",
    "        isinstance(pipeline_steps_df, pd.DataFrame)\n",
    "        and not pipeline_steps_df.empty\n",
    "        and isinstance(models_df, pd.DataFrame)\n",
    "        and not models_df.empty\n",
    "    ):\n",
    "        step_lookup = models_df.rename(\n",
    "            columns={\n",
    "                \"id\": \"model_id\",\n",
    "                \"name\": \"model_name\",\n",
    "                \"identifier\": \"model_identifier\",\n",
    "            }\n",
    "        )\n",
    "        pipeline_steps_detail = (\n",
    "            pipeline_steps_df.merge(step_lookup, on=\"model_id\", how=\"left\")\n",
    "            .sort_values([\"pipeline_id\", \"step_order\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        display(pipeline_steps_detail)\n",
    "\n",
    "    pipeline_metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
